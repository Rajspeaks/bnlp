{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "bnlp_colab_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN17wQ21BHS5Q6nGknIoiPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagorbrur/bnlp/blob/master/notebook/bnlp_colab_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BNLP\n",
        "\n",
        "BNLP is a natural language processing toolkit for Bengali Language. This tool will help you to tokenize Bengali text, Embedding Bengali words, Bengali POS Tagging, Construct Neural Model for Bengali NLP purposes.\n",
        "\n",
        "Here we are prodiving training approach of different model using **BNLP**"
      ],
      "metadata": {
        "id": "0SQ0x9bh9QsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "MuT4uyIf5-Gy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "!pip install -U bnlp_toolkit"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bnlp_toolkit\n",
            "  Downloading https://files.pythonhosted.org/packages/16/be/44d78b55ad8121cce1ca0bdbc7cf1db8d3f585006bacb08bd53ec8653957/bnlp_toolkit-3.0.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from bnlp_toolkit) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: gensim in /usr/local/lib/python3.6/dist-packages (from bnlp_toolkit) (3.6.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from bnlp_toolkit) (1.4.1)\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from bnlp_toolkit) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->bnlp_toolkit) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bnlp_toolkit) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (0.8.7)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (4.41.1)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bnlp_toolkit) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->bnlp_toolkit) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->bnlp_toolkit) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->bnlp_toolkit) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->bnlp_toolkit) (1.24.3)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, sentencepiece, bnlp-toolkit\n",
            "Successfully installed bnlp-toolkit-3.0.0 python-crfsuite-0.9.7 sentencepiece-0.1.91 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ],
      "metadata": {
        "id": "KJN642aj5nVc",
        "outputId": "20f88496-2e42-47e1-b70d-e4dca8037351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Downloading Bengali Processed Wikipedia Data "
      ],
      "metadata": {
        "id": "IWy0qUdy6BY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "#drive data download code\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "downloaded = drive.CreateFile({'id':\"1rQUQLsXg0TZnlrAgmNMkCXGDnYbjlLmM\"})\n",
        "downloaded.GetContentFile('bn_wiki_data.txt.zip')\n",
        "\n",
        "!unzip bn_wiki_data.txt.zip\n",
        "!rm -rf bn_wiki_data.txt.zip"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  bn_wiki_data.txt.zip\n",
            "  inflating: bn_wiki_data.txt        \n"
          ]
        }
      ],
      "metadata": {
        "id": "AcwFE8le5yTF",
        "outputId": "69cad5d1-3917-4376-bc81-ea1340cfd240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Here we present `bengali sentencepiece`, `bengali word2vec`, `bengali fasttext` training on `bengali wikipedia data`\n",
        "\n",
        "Training time will depend on data size."
      ],
      "metadata": {
        "id": "350KPo4D6Z4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Sentencepice Model\n",
        "\n",
        "After successfully compiling the below code will produce two file:\n",
        "\n",
        "* `wiki_sp.model` \n",
        "* `wiki_sp.vecab`"
      ],
      "metadata": {
        "id": "I_wHJFOW6dlo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "from bnlp import SentencepieceTokenizer\n",
        "\n",
        "bsp = SentencepieceTokenizer()\n",
        "data = \"bn_wiki_data.txt\"\n",
        "model_prefix = \"wiki_sp\"\n",
        "vocab_size = 30000\n",
        "bsp.train(data, model_prefix, vocab_size) "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "punkt not found. downloading...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "wiki_sp.model and wiki_sp.vocab is saved on your current directory\n"
          ]
        }
      ],
      "metadata": {
        "id": "8l7DUWI66MD4",
        "outputId": "d7710e45-6981-432e-96fb-9ec2b9c159ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Word2Vec Model\n",
        "\n",
        "After successfully compiling it will produce three file. \n",
        "\n",
        "* `wiki_word2vec.model`\n",
        "* `wiki_word2vec.vector`\n",
        "* `wiki_word2vec.model.trainables.syn1neg.npy`\n",
        "* `wiki_word2vec..model.wv.vectors.npy`\n"
      ],
      "metadata": {
        "id": "k-k4Dszo61v2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "from bnlp import BengaliWord2Vec\n",
        "bwv = BengaliWord2Vec()\n",
        "data_file = \"bn_wiki_data.txt\"\n",
        "model_name = \"wiki_word2vec.model\"\n",
        "vector_name = \"wiki_word2vec.vector\"\n",
        "bwv.train(data_file, model_name, vector_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wiki_word2vec.model and wiki_word2vec.vector saved in your current directory.\n"
          ]
        }
      ],
      "metadata": {
        "id": "OphHV5Yp60KW",
        "outputId": "7ce2a259-6339-494d-e023-5ffbe787c774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-training or resume Bengali word2vec training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from bnlp import BengaliWord2Vec\n",
        "bwv = BengaliWord2Vec()\n",
        "\n",
        "trained_model_path = \"mytrained_model.model\"\n",
        "data_file = \"raw_text.txt\"\n",
        "model_name = \"test_model.model\"\n",
        "vector_name = \"test_vector.vector\"\n",
        "bwv.pretrain(trained_model_path, data_file, model_name, vector_name, epochs=5)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Fasttext Model\n",
        "First of all install `fasttext` using `pip install fasttext` and restart runtime.\n",
        "\n",
        "After successfully training it will produce: \n",
        "* `wiki_fasttext.bin` "
      ],
      "metadata": {
        "id": "TAMgr4WT8x2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "!pip install fasttext"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3028144 sha256=a5be1db1a06d14a27544955a835aa8d643eb1d27e0bb84a81b965a513334aaf5\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ]
        }
      ],
      "metadata": {
        "id": "JXptOhxg4s6r",
        "outputId": "a9386ef0-032c-437e-c416-e34bce2b792e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "from bnlp.embedding.fasttext import BengaliFasttext\n",
        "\n",
        "bft = BengaliFasttext()\n",
        "data = \"bn_wiki_data.txt\"\n",
        "model_name = \"wiki_fasttext.bin\"\n",
        "epoch = 1\n",
        "bft.train(data, model_name, epoch)"
      ],
      "outputs": [],
      "metadata": {
        "id": "F67Yzdu08xBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali POS TAGGING CRF model\n",
        "\n",
        "After successfully training it will produce a trained model with accuracy on evaluation data: \n",
        "\n",
        "* `pos_model.pkl`"
      ],
      "metadata": {
        "id": "ZtsLVmOs9lgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from bnlp import POS\n",
        "bn_pos = POS()\n",
        "model_name = \"pos_model.pkl\"\n",
        "train_data = [[('রপ্তানি', 'JJ'), ('দ্রব্য', 'NC'), ('-', 'PU'), ('তাজা', 'JJ'), ('ও', 'CCD'), ('শুকনা', 'JJ'), ('ফল', 'NC'), (',', 'PU'), ('আফিম', 'NC'), (',', 'PU'), ('পশুচর্ম', 'NC'), ('ও', 'CCD'), ('পশম', 'NC'), ('এবং', 'CCD'),('কার্পেট', 'NC'), ('৷', 'PU')], [('মাটি', 'NC'), ('থেকে', 'PP'), ('বড়জোর', 'JQ'), ('চার', 'JQ'), ('পাঁচ', 'JQ'), ('ফুট', 'CCL'), ('উঁচু', 'JJ'), ('হবে', 'VM'), ('৷', 'PU')]]\n",
        "test_data = [[('রপ্তানি', 'JJ'), ('দ্রব্য', 'NC'), ('-', 'PU'), ('তাজা', 'JJ'), ('ও', 'CCD'), ('শুকনা', 'JJ'), ('ফল', 'NC'), (',', 'PU'), ('আফিম', 'NC'), (',', 'PU'), ('পশুচর্ম', 'NC'), ('ও', 'CCD'), ('পশম', 'NC'), ('এবং', 'CCD'),('কার্পেট', 'NC'), ('৷', 'PU')], [('মাটি', 'NC'), ('থেকে', 'PP'), ('বড়জোর', 'JQ'), ('চার', 'JQ'), ('পাঁচ', 'JQ'), ('ফুট', 'CCL'), ('উঁচু', 'JJ'), ('হবে', 'VM'), ('৷', 'PU')]]\n",
        "\n",
        "bn_pos.train(model_name, train_data, test_data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "Training Started........\n",
            "it will take time according to your dataset size..\n",
            "Training Finished!\n",
            "Evaluating with Test Data...\n",
            "Accuracy is: \n",
            "0.1111111111111111\n",
            "Model Saved!\n"
          ]
        }
      ],
      "metadata": {
        "id": "VUKhbkaBE-CV",
        "outputId": "e3cd7857-9fec-42dc-d2c2-f037c3ab55f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Bengali NER model\n",
        "After successfully training it will produce a trained model with accuracy on evaluation data:\n",
        "\n",
        "* `ner_model.pkl` "
      ],
      "metadata": {
        "id": "dPB7SBrKuSna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "from bnlp import NER\n",
        "bn_ner = NER()\n",
        "model_name = \"ner_model.pkl\"\n",
        "train_data = [[('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')]]\n",
        "test_data = [[('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')]]\n",
        "\n",
        "bn_ner.train(model_name, train_data, test_data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "1\n",
            "Training Started........\n",
            "It will take time according to your dataset size...\n",
            "Training Finished!\n",
            "Evaluating with Test Data...\n",
            "Accuracy is: \n",
            "1.0\n",
            "Model Saved!\n"
          ]
        }
      ],
      "metadata": {
        "id": "of_1lkdW917n",
        "outputId": "b3d54074-cda1-46d9-b4f9-800c50e4ef18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "qVrYxT5DulwP"
      }
    }
  ]
}